additionalPrometheusRulesMap:
  rule-name:
    groups:
    - name: kube-state-metrics 
      rules:
        - alert: KubernetesPodNotHealthy
          expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-running state for longer than 1 minute.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"

        - alert: KubernetesDaemonsetRolloutStuck
          expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})"
            description: "Some Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled or not ready\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"

        - alert: ContainerHighCpuUtilization
          expr: (sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod, container) / sum(container_spec_cpu_quota{container!=""}/container_spec_cpu_period{container!=""}) by (pod, container) * 100) > 80
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Container High CPU utilization (instance {{ $labels.instance }})"
            description: "Container CPU utilization is above 80%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"

        - alert: ContainerHighMemoryUsage
          expr: (sum(container_memory_working_set_bytes{name!=""}) BY (instance, name) / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) * 100) > 80
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Container High Memory usage (instance {{ $labels.instance }})"
            description: "Container Memory usage is above 80%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"

        - alert: KubernetesContainerOomKiller
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes Container OOM killer (instance {{ $labels.instance }})"
            description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"

        - alert: KubernetesPodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}"

        - alert: KubernetesNodeNotReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes Node not ready (instance {{ $labels.instance }})
            description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: KubernetesNodeMemoryPressure
          expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes Node memory pressure (instance {{ $labels.instance }})
            description: "Node {{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        
        - alert: KubernetesStatefulsetDown
          expr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes StatefulSet down (instance {{ $labels.instance }})
            description: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} went down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - name: argocd
      rules:
        - alert: ArgocdServiceNotSynced
          expr: argocd_app_info{sync_status!="Synced"} != 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: ArgoCD service not synced (instance {{ $labels.instance }})
            description: "Service {{ $labels.name }} run by argo is currently not in sync.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: ArgocdServiceUnhealthy
          expr: argocd_app_info{health_status!="Healthy"} != 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: ArgoCD service unhealthy (instance {{ $labels.instance }})
            description: "Service {{ $labels.name }} run by argo is currently not healthy.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: false
    configReloaders: true
    general: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: false
    k8sPodOwner: false
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubeControllerManager: false
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: true
    kubeSchedulerRecording: true
    kubeStateMetrics: true
    network: false
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: false

alertmanager:
  config:
    global:
      resolve_timeout: 1m
      slack_api_url: 'https://hooks.slack.com/services/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'

    route:
      receiver: 'slack'
      repeat_interval: 1s
      routes:
        - matchers:
            - severity="critical"
          receiver: 'slack'
          continue: true

    receivers:
    - name: 'slack'
      slack_configs:
      - channel: '#infra-alerts'
        send_resolved: true

kubeControllerManager:
  enabled: false
kubeEtcd:
  enabled: false
kubeScheduler:
  enabled: false

grafana:
  additionalDataSources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki-distributed:3100
    isDefault: false
    jsonData:
      maxLines: 1000